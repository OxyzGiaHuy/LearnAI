### Annotation
- scalar -> ordinary lower-cased letter -> 0d
- vector -> bold lower-cased letter -> 1d
- matrix -> bold upper-cased letter -> 2d
- tensor -> nd

### Operations
- hadamard product -> element wise product
- sum is reduction: sum along an axis will reduce that axis
- non-reduction sum: ```.sum(axis=1, keepdims=True)```
- cummulative sum along an axis _a_: ```.cumsum(axis=a)```
- dot product (inner product $\langle \mathbf{x}, \mathbf{y}\rangle$): $\mathbf{x}^\top \mathbf{y}$
- matrix--vector product: ```torch.mv(A,x)``` or ```A @ x```
(column dimension of A must equal length of x)
- matrix--matrix multi: 
    - ```torch.mm(A, B)``` or ```A @ B```
    - = $m$ matrix--vector or $m \times n$ dot product
- $l_2$ norm: ```torch.norm(x)```
- the *Frobenius norm* ($l_2$ norm of matrix-shaped vector) defined as the square root of the sum of the squares of a matrix's elements:
**$$\|\mathbf{X}\|_\textrm{F} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$**
-> norms are what we need to solve optimization problems

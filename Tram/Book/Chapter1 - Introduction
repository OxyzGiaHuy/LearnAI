Notation explanations
Numbers and Arrays:
	A tensor: analogous to but more general than a vector, an array of components that are functions of the coordinates of a space.
	e^(i): standard basis vector with 1 at pos i
	diag(a): a square with diag entries given by a
	italic: specific, normal: random variable
	
Sets and Graphs:
	G: A graph
	Pa_G(xi): The parents of xi in G
	
Indexing: starting at 1
	a-i: all elements of vector a except for ele i
	use : like Python to slice
	A_(:,:,i): 2-D slice of a 3-D tensor

Linear Algebra Operations:
	A^(+): Moore-Penrose pseudoinverse of A (*)
	A . B: Element-wise (Hadamard)
	
Calculus:
	∂f/∂x: Jacobian matrix (ma tran cac dao ham cap 1)
	∇2f/d(xixj): Hessian matrix (ma tran cac dao ham cap 2)

Probability and Information Theory
	a⊥b: random vars a and b are independent
	a⊥b|c: a and b conditionally independent given c
	P(a): probability distribution over a discrete var
	p(a): probability distribution over a continuous var or type has not been specified
	a ~ P: rand a has distribution P
	Ef(x): Expectation of f(x) with respect to P(x), x ~ P
	Var(f(x)): Variance of f(x) under P(x)
	Cov(f(x), g(x)): Covariance of f(x) and g(x) under P(x)
	H(x): Shannon entropy of rand var x, measures uncertaincy of a prob
	D_(KL)(P || Q): Kullback-Leibler divergence of P and Q
	N(x;µ,Σ): Gaussian distribution over x with mean µ and covariance Σ
	
Good web: https://thetalog.com/statistics/ly-thuyet-thong-tin/
	
-> Sớm update!
??
?? 

Chapter 1: Introduction

* The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally—problems that we solve intuitively, that feel automatic, like recognizing spoken words or faces in images.
* The one solution: learn from experience, understand the world in terms of a hierachy of concepts to simpler concepts.
----> Deep learning
Most famous Prjs: Cyc (Lenat and Guha, 1989) - CycL, but failed.

AI systems need the ability to acquire their own knowledge, by extracting patterns from raw data.
----> Machine learning.
Simple ML algo: logistic regression, naive Bayes.

Representation - Feature - Structured and indexed intelligently
* Representation learning itself:
	- Autoencoder
	- Factors of variation explain the observed data
--> Require us to disentangle the factors of variation and discard the ones that we do not care about.

Quintessential: 
	* Multilayer perceptron (MLP): visible layer, hidden layer
Cybernetics - Connectionism - Neural networks

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section offers a gentle introduction to the most essential concepts, starting from scalar arithmetic and ramping up to matrix multiplication*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5., 4.]),\n",
       " tensor([6., 4.]),\n",
       " tensor([1.5000, 1.0000]),\n",
       " tensor([9., 4.]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([3.0, 2.0])\n",
    "y = torch.tensor(2.0)\n",
    "x+y, x*y, x/y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) torch.Size([3]) torch.Size([3])\n",
      "dimVector = 3 and x.shape = 3x1\n"
     ]
    }
   ],
   "source": [
    "# torch.arange(start, end, step)\n",
    "x = torch.arange(0,3,1)\n",
    "print(x, x.shape, x.T.shape)\n",
    "print(f\"dimVector = {len(x)} and x.shape = 3x1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 1, 2],\n",
      "        [0, 2, 4]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange(3).reshape(3, 1)\n",
    "print(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "tensor([[0, 2, 4],\n",
      "        [1, 3, 5]])\n",
      "tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(6).reshape(3, 2)\n",
    "print(A)\n",
    "print(A.T)\n",
    "\n",
    "# Symmetric matrices A = A.T\n",
    "A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "print(A)\n",
    "print(A==A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4. Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors give us a generic way of describing extensions to n_th-order arrays\n",
    "torch.arange(24).reshape([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5. Basic Properties of Tensor Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "B = A.clone()  # Assign a copy of A to B by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A.shape, B.shape)\n",
    "# elementwise product of two matrices, Hadamard product\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6. Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor(3.))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(3, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " torch.Size([2]),\n",
       " tensor([[0., 1., 2.],\n",
       "         [3., 4., 5.]]),\n",
       " tensor([ 3., 12.]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A.shape, A.sum(axis=1).shape)\n",
    "A.shape, A.sum(axis=1).shape, A, A.sum(axis=1) # -> axis = 1, keep col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000) tensor(2.5000) 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A.mean(), A.sum() / A.numel(), A.shape[0])\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0] # row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7. Non-Reduction Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.],\n",
       "         [12.]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A, sum_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.6667],\n",
       "        [0.2500, 0.3333, 0.4167]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8]],\n",
      "\n",
      "        [[ 9, 10, 11],\n",
      "         [12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23],\n",
      "         [24, 25, 26]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 11, 13],\n",
       "         [15, 17, 19],\n",
       "         [21, 23, 25]],\n",
       "\n",
       "        [[27, 30, 33],\n",
       "         [36, 39, 42],\n",
       "         [45, 48, 51]]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tA = torch.arange(27).reshape([3, 3, 3])\n",
    "print(tA)\n",
    "\n",
    "# row by row, first shape is instill\n",
    "tA.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.8. Dot Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given two vectors x, y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.]) tensor([True, True, True])\n",
      "tensor(3.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.dot(x, y) = sum (x.T * y)\n",
    "y = torch.ones(3, dtype = torch.float32)\n",
    "\n",
    "# vector: x = x.T\n",
    "print(x.T, x==x.T)\n",
    "print((x.T * y).sum())\n",
    "x, y, torch.dot(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.9. Matrix–Vector Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given mxn matrix and n-dim vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x), A@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = 1x3, tA = 3x1\n",
    "tA = torch.arange(3, dtype=torch.float32).reshape(3, 1)\n",
    "x@tA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.10. Matrix–Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]),\n",
       " tensor([[ 3.,  3.,  3.,  3.],\n",
       "         [12., 12., 12., 12.]]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(3, 4)\n",
    "torch.mm(A, B), A@B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.11. Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vector $ \\bold{x}: l_p = \\|x\\|_p = (\\sum_{i=1}^{n} |x_i|^p)^{1/p}$\n",
    "- Frobenius norm - Matrix $ \\bold{X}:  \\|X\\|_F = \\sqrt(\\sum_{i=1}^{m}\\sum_{j=1}^{n}x_{ij}^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.ones((4, 9)))\n",
    "\n",
    "# sqrt(4*9*1^2)\n",
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, we are often trying to solve optimization problems\n",
    "- maximize the probability assigned to observed data;\n",
    "\n",
    "- maximize the revenue associated with a recommender model; \n",
    "\n",
    "- minimize the distance between predictions and the ground truth observations; \n",
    "\n",
    "- minimize the distance between representations of photos of the same person while maximizing the distance between representations of photos of different people.\n",
    "\n",
    "- these distances, which constitute the objectives of deep learning algorithms, are often expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.12. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.13. Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 1: Prove (A.T).T == A\n",
    "A = torch.rand(2, 3)\n",
    "(A.T).T == A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True],\n",
       "        [True, True],\n",
       "        [True, True]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 2: A.T + B.T == (A+B).T\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(2, 1)\n",
    "A.T + B.T == (A+B).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 3: Is A + A.T symmetric?\n",
    "A = torch.randint(3, 5, (3, 3))\n",
    "T = A + A.T\n",
    "T == T.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 1, 1],\n",
      "         [1, 0, 1, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [1, 1, 1, 1],\n",
      "         [0, 0, 1, 0]]])\n",
      "tensor([[0, 0, 1, 1],\n",
      "        [2, 1, 2, 1],\n",
      "        [0, 0, 1, 0]])\n",
      "tensor([2, 6, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 4: len(X) = 2 (WOW)\n",
    "X = torch.randint(0, 2, (2, 3, 4))\n",
    "print(X)\n",
    "print(X.sum(axis = 0))\n",
    "print(X.sum(axis = 0).sum(axis = 1))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 4],\n",
      "        [4, 3, 4],\n",
      "        [4, 4, 3]]) tensor([10, 11, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3000, 0.2727, 0.3636],\n",
       "        [0.4000, 0.2727, 0.3636],\n",
       "        [0.4000, 0.3636, 0.2727]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 5 -> len(X) always correspond to the length of axis 0 -> the first dim of matrix\n",
    "\n",
    "# Ex 6: Run A / A.sum(axis = 1)\n",
    "# shape: row x col, sum -> asix, keep only that attributor -> sum by other attributor\n",
    "print(A, A.sum(axis = 1)) # sum by row\n",
    "A / A.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Ex 7: Travel in Manhattan -> use Norm L1 = |x1 - x2| + |y1 - y2|\n",
    "# Ex 8: Sum (2, 3, 4): -> 2 + 3 + 4 = 9\n",
    "X = torch.randint(0, 2, (2, 3, 4))\n",
    "print(X.shape[0] + X.shape[1] + X.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Ex 9: Norm L2\n",
    "X = torch.arange(0,9,dtype=torch.float).reshape(3, 3)\n",
    "print(X)\n",
    "res = 0\n",
    "for i in X:\n",
    "    for j in i:\n",
    "        res += j * j\n",
    "\n",
    "print(torch.linalg.norm(X) == res**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1974, 0.0648, 0.2604],\n",
      "        [0.6919, 0.7498, 0.0917]]) \n",
      " tensor([[0.3804, 0.9647],\n",
      "        [0.0453, 0.8638],\n",
      "        [0.2342, 0.0497]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1390, 0.2593],\n",
       "        [0.3186, 1.3197]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 10: A in R^{2^10 x 2^16}, B in R^{2^16 x 2^5}, C in R^{2^5 x 2^14}\n",
    "'''\n",
    "m x n, n x p\n",
    "torch.mm(A, B) or A@B\n",
    "-> compute (m * p) * (n muls + n adds)\n",
    "\n",
    "-> AB in R^{2^10 x 2^5}, C in R^{2^5 x 2^14}\n",
    "-> A in R^{2^10 x 2^16}, BC in R^{2^16 x 2^14}\n",
    "\n",
    "=> O((AB)C) = 2^(10 + 14 + 2*5) = 2^34 \n",
    "=> O(A(BC)) = 2^(10 + 5 + 2*16) = 2^47\n",
    "    \n",
    "'''\n",
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 2)\n",
    "Ans = torch.zeros((2, 2))\n",
    "print(A, \"\\n\", B)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        Ans[i][j] = 0\n",
    "        for k in range(3):\n",
    "            Ans[i][j] += A[i][k] * B[k][j]\n",
    "print(Ans == torch.mm(A, B))\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1512,  8.3738,  6.8723,  7.5550,  6.4670,  8.4816,  8.0702,  6.3920,\n",
       "          7.2685,  8.2481,  8.4397,  6.4160,  7.5176,  8.1775,  8.5091,  6.7916,\n",
       "          7.0622,  7.5079,  7.0922,  7.4095,  7.0315,  8.4757,  8.7261,  6.9682,\n",
       "          9.5741,  8.0055,  6.7795,  8.6091,  8.8621,  8.3429,  6.6739,  8.3045,\n",
       "          8.4317,  7.6374,  6.3798,  6.7931,  7.8707,  7.5754,  7.8255,  8.9366],\n",
       "        [10.3036,  9.5763,  7.7581,  8.3642,  7.5685,  9.1082,  7.9427,  6.9862,\n",
       "          9.3636,  8.7989,  8.7772,  9.0248,  7.8074, 10.1574, 10.7980,  8.3722,\n",
       "          7.4362,  8.1263,  8.0210,  9.9596, 10.0800, 10.0313,  9.8257,  6.9680,\n",
       "         10.6238, 10.1832,  8.3637,  9.3602,  9.9949,  9.2636,  8.9047,  9.2404,\n",
       "          9.3598,  8.9625,  8.7506,  8.7501,  9.5070,  8.7402,  9.5386,  8.3707],\n",
       "        [ 8.9740,  8.7335,  7.0516,  7.9406,  7.2479,  8.3149,  7.8493,  6.7739,\n",
       "          9.0226,  8.4924,  9.0769,  8.5185,  7.9147,  9.0327,  9.9033,  7.4677,\n",
       "          7.2303,  8.0544,  8.4016,  9.1976,  8.8976,  9.1390,  9.2383,  6.5247,\n",
       "          9.5219,  9.1601,  7.8977,  9.0924, 10.1172,  9.4233,  8.4560, 10.2277,\n",
       "         10.1485,  9.6880,  7.8354,  7.6476,  8.5122,  7.4313,  8.0398,  8.7242],\n",
       "        [ 9.1793,  9.1481,  7.4586,  7.8506,  7.1587,  9.1845,  6.9137,  6.9001,\n",
       "          8.3963,  8.2241,  7.5935,  7.2851,  6.3335,  9.0986,  9.0586,  7.7052,\n",
       "          8.1041,  8.4739,  7.6952,  7.8612,  7.9287,  8.9456,  8.9875,  6.9521,\n",
       "          9.3641,  8.7089,  7.1619,  8.0999,  9.2833,  7.9965,  7.4235,  8.2358,\n",
       "          7.9641,  7.2670,  6.8796,  7.1737,  9.3004,  8.3584,  8.9194,  8.7757],\n",
       "        [ 9.1785,  9.4189,  7.4231,  8.0762,  7.1074,  9.0320,  7.2703,  7.0710,\n",
       "          8.2194,  8.6053,  7.9474,  8.0599,  7.5305,  9.3816,  9.1639,  7.4257,\n",
       "          8.0910,  8.2517,  7.9440,  8.5263,  8.6720,  9.2396,  8.9783,  6.3112,\n",
       "          9.2043,  8.6891,  8.1808,  8.8940,  9.9074,  9.0107,  8.4322,  9.0751,\n",
       "          8.0437,  8.5534,  6.9484,  7.8863,  8.5844,  8.1096,  9.0653,  8.6603],\n",
       "        [ 7.9586,  7.1432,  6.4275,  7.4054,  6.1974,  7.8085,  7.2110,  5.9658,\n",
       "          6.6734,  6.8902,  6.8918,  6.3387,  5.8892,  7.7797,  7.5479,  6.6972,\n",
       "          7.0854,  5.8270,  6.2244,  7.0745,  6.9057,  7.9032,  7.2301,  5.6751,\n",
       "          9.1175,  7.0864,  5.8407,  7.8591,  7.3652,  7.1357,  6.6153,  7.6636,\n",
       "          6.7389,  6.3490,  6.2061,  6.3681,  7.7228,  7.4524,  8.0634,  6.4074],\n",
       "        [ 6.9330,  6.1172,  5.0557,  5.7021,  4.9021,  6.3970,  6.0333,  5.8284,\n",
       "          5.8119,  6.0648,  6.8735,  6.7958,  4.8242,  6.4196,  7.5431,  5.1867,\n",
       "          6.1249,  5.4228,  5.8135,  6.9510,  5.5584,  7.1072,  6.6655,  4.9490,\n",
       "          6.9641,  7.1698,  5.9637,  6.7027,  7.9806,  6.2869,  5.8761,  6.9064,\n",
       "          6.8954,  6.1396,  5.7951,  5.7047,  6.3349,  5.6465,  6.3660,  5.7515],\n",
       "        [ 6.7693,  7.0812,  5.3973,  6.7774,  5.4008,  6.2405,  7.4181,  6.1954,\n",
       "          5.8964,  7.3566,  7.0362,  5.8857,  6.5325,  7.2450,  7.4212,  5.5298,\n",
       "          6.4053,  5.3383,  5.3655,  7.0986,  6.4117,  7.7594,  7.2232,  5.4158,\n",
       "          7.7947,  8.0063,  6.4886,  7.4105,  8.3004,  6.9782,  5.7441,  7.4046,\n",
       "          7.1795,  7.3326,  6.0887,  6.3174,  6.5057,  5.9414,  6.9309,  6.7618],\n",
       "        [ 7.7443,  7.8036,  5.4903,  6.7062,  5.5938,  6.6566,  5.6603,  6.3815,\n",
       "          7.4500,  6.5581,  6.7840,  7.3503,  6.6370,  8.0595,  8.6530,  7.2286,\n",
       "          6.5520,  7.3590,  7.1380,  7.9961,  7.7680,  7.9635,  7.9471,  5.3370,\n",
       "          8.4054,  7.2965,  7.1204,  7.0090,  8.6632,  7.9864,  6.9176,  8.8604,\n",
       "          7.5771,  7.7326,  6.3858,  6.4370,  7.2815,  6.6141,  8.1117,  7.6686],\n",
       "        [ 9.1580,  8.5376,  6.5438,  7.9375,  6.8905,  8.0180,  7.4364,  6.7257,\n",
       "          7.5536,  7.3854,  7.7041,  7.1674,  6.9575,  8.1125,  9.4696,  7.0784,\n",
       "          6.2944,  7.2896,  7.3593,  8.3304,  8.4389,  8.9201,  8.8745,  6.4854,\n",
       "          9.1263,  8.6545,  7.8205,  8.1071,  8.7434,  8.0473,  7.6734,  8.7786,\n",
       "          8.2320,  7.9062,  6.8890,  7.2324,  8.0656,  7.9525,  8.5452,  6.8890],\n",
       "        [ 7.6075,  6.9039,  5.6529,  5.9646,  6.7306,  6.9571,  5.7699,  6.1553,\n",
       "          6.2764,  6.0607,  7.2880,  5.6519,  5.9351,  7.5240,  7.5293,  6.8805,\n",
       "          6.5863,  7.5032,  5.7394,  7.6244,  6.3933,  7.3166,  8.1521,  5.2365,\n",
       "          7.8059,  7.5936,  5.7179,  7.8995,  8.2185,  7.8652,  6.8836,  7.4291,\n",
       "          7.5035,  7.2817,  6.7746,  7.2593,  7.9344,  6.4546,  8.5654,  6.9050],\n",
       "        [ 9.1646,  8.7364,  7.0320,  8.1031,  7.3801,  8.4025,  7.5989,  7.3690,\n",
       "          7.3206,  7.5281,  8.1270,  8.0958,  8.7846,  9.3593,  9.8583,  8.7269,\n",
       "          7.9881,  8.2444,  7.0920,  8.3532,  9.1846,  9.0275,  9.6187,  6.6594,\n",
       "         10.0506,  8.5358,  7.9309,  8.7170,  9.9512,  9.5501,  8.0861,  9.5851,\n",
       "          8.6454,  8.6645,  8.2386,  7.9749,  8.5290,  8.6345,  9.5162,  8.1299],\n",
       "        [ 7.9754,  7.4460,  5.6294,  6.8176,  6.0226,  7.2396,  6.1964,  6.4912,\n",
       "          7.3814,  6.9662,  6.4699,  5.8038,  5.6858,  7.5118,  7.8193,  6.5183,\n",
       "          6.0235,  7.5680,  5.9504,  7.9044,  7.8022,  7.4386,  7.8482,  5.8749,\n",
       "          7.1019,  6.9438,  6.4915,  6.6829,  7.8568,  7.0531,  6.5883,  6.4405,\n",
       "          6.7558,  6.5672,  6.2583,  6.1423,  7.0094,  7.1527,  7.1935,  6.8556],\n",
       "        [ 8.0554,  7.4137,  5.9609,  6.9905,  6.4738,  8.2061,  7.3798,  6.3734,\n",
       "          7.7291,  7.1372,  7.9991,  7.7190,  6.6285,  8.0490,  9.1793,  6.9849,\n",
       "          6.9597,  7.0419,  6.9975,  8.3162,  8.0656,  8.4507,  8.3018,  6.6861,\n",
       "          9.0169,  8.2614,  7.1607,  7.8480,  8.9173,  8.4294,  7.2084,  8.8823,\n",
       "          7.7512,  8.4658,  7.2727,  6.9391,  7.5825,  6.6651,  8.4797,  7.8678],\n",
       "        [10.8092,  9.7179,  8.3641, 10.4579,  8.3623, 10.3378,  8.6174,  7.6392,\n",
       "          9.5789,  9.2236,  9.8140,  8.8652,  9.2316, 10.8007, 10.7428,  9.1576,\n",
       "          8.6571,  9.0056,  8.1844,  9.5407,  9.6518, 10.5565, 10.3690,  8.3326,\n",
       "         10.4296,  9.6021,  8.5966,  9.3057, 11.3031, 10.0835,  8.5735, 10.3656,\n",
       "         10.5950, 10.0732,  8.3059,  8.7691, 10.9566,  9.5114, 10.2916,  9.5159],\n",
       "        [ 9.4461,  9.3421,  7.9575,  8.2396,  7.1249,  9.1268,  7.5023,  7.6844,\n",
       "          8.0167,  7.7008,  8.0459,  7.9030,  7.2016,  9.3424,  9.1156,  8.0704,\n",
       "          8.2743,  7.9997,  7.9677,  8.3424,  8.9165,  9.8824,  9.3817,  7.0106,\n",
       "          8.9672,  8.6845,  8.2273,  8.1667,  9.8216,  8.4615,  7.8680,  9.1045,\n",
       "          8.7167,  7.9794,  7.5980,  7.2090,  9.1221,  8.8867, 10.0387,  8.5989],\n",
       "        [ 9.7813,  8.8441,  6.7583,  8.0550,  6.9083,  8.7988,  7.2662,  7.7054,\n",
       "          8.9360,  8.3557,  8.6642,  9.2385,  7.3237,  9.6225, 10.3926,  7.9403,\n",
       "          8.4300,  8.0343,  7.6834,  9.3012,  8.4488,  9.9453,  9.2345,  7.2174,\n",
       "         10.1237,  9.0353,  7.5492,  8.6901,  9.9792,  8.6961,  8.0500,  9.4463,\n",
       "          8.5030,  8.2432,  8.8051,  8.0555,  9.5884,  9.0916,  9.5597,  8.0709],\n",
       "        [ 9.0920,  8.6971,  7.1388,  8.7077,  6.7816,  9.0547,  7.7896,  6.7371,\n",
       "          7.6152,  7.5878,  7.7827,  7.6030,  7.7335,  9.1312,  9.3012,  7.5915,\n",
       "          7.2557,  7.8598,  6.9205,  7.9775,  8.4602,  8.9153,  9.1510,  7.4943,\n",
       "          9.7988,  7.6025,  7.5502,  8.6473,  9.4631,  8.4790,  7.1961,  8.8249,\n",
       "          8.3286,  7.6256,  6.9410,  8.3697,  8.4307,  8.5564,  9.4393,  7.7750],\n",
       "        [ 7.6755,  6.6959,  6.1863,  7.2005,  5.5066,  6.8428,  6.0961,  5.2102,\n",
       "          6.6608,  6.8618,  6.0916,  6.7767,  6.6741,  8.0247,  8.4837,  7.2937,\n",
       "          6.1415,  6.0803,  5.8960,  7.6881,  6.9534,  7.9912,  7.5091,  5.3154,\n",
       "          9.0106,  7.4465,  6.0929,  7.4869,  8.0391,  7.1260,  6.3414,  8.1547,\n",
       "          6.8237,  6.6279,  6.5625,  7.2875,  7.0691,  6.6110,  7.2361,  6.2057],\n",
       "        [ 9.2404,  8.0148,  6.1295,  6.8950,  6.2185,  8.3793,  7.0391,  7.0759,\n",
       "          7.3613,  7.1087,  7.0530,  8.0167,  6.0095,  8.1323,  8.7670,  6.3252,\n",
       "          7.2295,  7.2879,  7.3695,  8.1258,  7.1078,  8.9864,  8.2394,  7.0428,\n",
       "          9.4313,  8.3318,  6.9489,  8.4846,  8.7877,  7.9685,  6.8627,  8.7195,\n",
       "          8.4657,  7.8303,  7.5330,  7.5164,  8.3051,  7.5396,  8.6148,  7.3270]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 11 -> Slightly different\n",
    "%time\n",
    "A = torch.rand(20, 30)\n",
    "B = torch.rand(30, 40)\n",
    "A@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "(1460, 11808)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.0020,  7.1044,  8.0900, 10.2174,  7.8628,  7.9131,  8.1432,  7.8910,\n",
       "          7.5302, 10.4786,  9.5091,  9.7601,  8.9581,  9.2501,  8.3976, 10.2384,\n",
       "          8.3071,  8.1910,  8.7037,  8.5819,  9.1828,  8.4556, 10.0694,  9.4204,\n",
       "          7.4288,  9.2304,  8.1929,  8.9815,  7.6031,  8.5688, 10.0139,  9.8744,\n",
       "          7.4123,  6.3364,  9.1461,  9.2136,  9.1498,  6.9270, 10.1612,  7.7086],\n",
       "        [ 7.3869,  6.3785,  6.5397,  7.7877,  6.6173,  7.0034,  6.2168,  5.9456,\n",
       "          6.5489,  7.7066,  7.5706,  7.6930,  7.3179,  7.5374,  6.7183,  6.3627,\n",
       "          6.1777,  6.6612,  6.6948,  7.3187,  8.1783,  6.1424,  7.7090,  6.9271,\n",
       "          5.9810,  8.0049,  7.4158,  7.9416,  6.1705,  6.9888,  7.8281,  6.5267,\n",
       "          5.7627,  5.6432,  7.1772,  6.7039,  8.4846,  5.3190,  7.1828,  6.5347],\n",
       "        [ 8.9471,  6.5951,  7.6864,  9.6427,  7.3254,  7.0270,  6.5421,  7.3427,\n",
       "          6.4209,  9.0225,  8.9264,  8.4371,  8.1694,  8.9754,  8.4848,  7.8483,\n",
       "          7.1690,  8.1201,  7.2950,  7.9620,  8.3736,  6.8660,  8.7690,  8.1018,\n",
       "          6.3261,  8.4793,  7.6082,  7.5255,  6.8022,  7.5850,  8.8611,  8.2942,\n",
       "          7.1768,  5.9637,  9.1607,  7.0240,  8.3923,  6.2531,  7.8560,  6.7175],\n",
       "        [ 6.1948,  6.0584,  5.4525,  6.3100,  4.8030,  5.8908,  7.4524,  5.8677,\n",
       "          5.1162,  7.1583,  6.7066,  6.3090,  5.2807,  5.9171,  5.9530,  6.4405,\n",
       "          5.1443,  6.5440,  6.2198,  6.9050,  6.5408,  5.1586,  7.1122,  6.8488,\n",
       "          5.2413,  7.0446,  6.0014,  6.4916,  6.1169,  6.3296,  6.8972,  6.4364,\n",
       "          5.8064,  4.9557,  7.0180,  6.4787,  6.0292,  4.6026,  7.5985,  5.2963],\n",
       "        [ 7.5619,  6.3493,  6.6821,  8.1260,  7.7964,  6.7965,  6.7358,  6.8697,\n",
       "          5.9375,  8.6822,  8.1516,  8.6382,  7.9461,  7.9372,  7.3696,  7.8624,\n",
       "          6.7569,  6.9399,  7.3609,  7.5981,  8.0817,  6.9546,  8.5816,  8.4113,\n",
       "          6.8942,  8.0038,  7.5689,  7.8583,  6.6738,  7.0034,  9.1779,  8.0675,\n",
       "          6.6741,  5.0725,  7.8554,  6.7099,  7.9694,  5.5645,  8.1218,  7.0755],\n",
       "        [ 6.3094,  5.1259,  5.8581,  8.0427,  6.5877,  5.4947,  6.4915,  5.6010,\n",
       "          5.8649,  6.8763,  7.2863,  7.3629,  7.0216,  6.9379,  6.3698,  6.5517,\n",
       "          5.8490,  6.1679,  6.4274,  6.4715,  7.4463,  6.0290,  7.3089,  6.7106,\n",
       "          5.7062,  7.2646,  6.8978,  6.8007,  6.4218,  7.1612,  6.9434,  6.4463,\n",
       "          6.3154,  4.3967,  7.1357,  6.1332,  7.7024,  4.2757,  5.9941,  6.2292],\n",
       "        [ 7.1738,  6.1141,  6.4878,  7.9625,  7.0800,  7.2143,  6.6587,  5.8733,\n",
       "          6.2923,  8.5713,  7.9130,  7.9628,  8.1977,  7.3640,  5.9465,  8.1634,\n",
       "          6.5136,  6.6514,  7.4564,  8.6755,  8.2539,  7.4829,  8.2847,  9.0614,\n",
       "          6.9786,  7.9685,  7.6084,  8.1556,  7.3999,  7.5127,  7.9070,  7.7989,\n",
       "          6.8201,  5.2909,  7.5219,  6.3694,  8.2774,  4.8853,  7.7653,  7.1661],\n",
       "        [ 9.1606,  7.1365,  8.7928, 11.2534,  9.2521,  8.0129,  9.3295,  8.9382,\n",
       "          7.6749, 10.6416, 10.5771, 10.4806,  9.9584,  9.8521,  8.4607, 10.3416,\n",
       "          8.5018,  8.9029, 10.3511, 10.3433, 10.2713,  8.8696, 10.8344, 10.4722,\n",
       "          7.9442,  9.5710,  9.9377,  9.6002,  9.5960,  9.2537, 11.8402, 10.0179,\n",
       "          8.5048,  6.8836, 10.0002,  9.1789, 10.1953,  8.2283, 10.3643,  8.5455],\n",
       "        [ 6.8281,  5.4071,  6.0981,  8.1333,  6.9941,  6.4809,  7.3229,  6.8417,\n",
       "          5.4720,  8.1749,  7.3009,  8.1427,  7.5415,  6.8461,  6.6397,  8.7129,\n",
       "          7.0510,  6.0598,  7.3407,  7.0594,  7.9064,  7.2470,  8.6271,  8.0252,\n",
       "          6.1268,  7.1633,  7.2029,  7.7352,  5.9236,  6.6409,  8.9721,  7.5891,\n",
       "          6.1772,  5.0224,  6.7161,  7.0111,  8.1425,  5.8496,  7.4256,  6.8740],\n",
       "        [ 6.3938,  6.4915,  6.6722,  8.1025,  5.7296,  6.4098,  6.5218,  5.9074,\n",
       "          5.5440,  8.3904,  7.0109,  7.3645,  6.8096,  6.9507,  5.6469,  8.3443,\n",
       "          6.0439,  6.8369,  6.4514,  7.0327,  6.7338,  6.5990,  6.9379,  7.6575,\n",
       "          5.7321,  7.1487,  7.0755,  6.7215,  6.8245,  6.6724,  7.4955,  7.1881,\n",
       "          6.8842,  5.2212,  6.8927,  6.2040,  6.9426,  5.1811,  6.6010,  6.3432],\n",
       "        [ 8.2089,  5.9551,  7.0054,  8.4278,  6.6065,  6.2610,  7.4485,  7.4825,\n",
       "          6.0728,  8.7998,  8.4855,  8.2490,  7.8182,  7.7964,  7.2979,  7.6650,\n",
       "          7.1615,  7.2638,  7.9510,  8.2788,  8.2641,  7.9941,  9.3195,  8.5952,\n",
       "          6.1567,  7.8770,  7.1607,  7.4214,  7.2168,  6.7942,  8.8359,  7.7914,\n",
       "          6.5933,  5.6319,  7.5991,  7.0020,  8.3457,  5.5258,  7.9510,  6.1728],\n",
       "        [ 9.6687,  8.1433,  8.6552,  9.6843,  8.8879,  9.5984, 10.2601,  7.7474,\n",
       "          7.5103, 10.2681, 11.9978, 10.4931,  9.9595,  9.5146,  9.5242, 10.4548,\n",
       "          7.9648,  9.9722,  9.3115, 11.3771, 10.9924,  8.6130, 11.6891, 10.2635,\n",
       "          8.9185, 11.5108,  9.2986, 10.6737,  8.8301,  9.5827, 10.9175, 10.6701,\n",
       "          9.2080,  7.1414, 10.5459,  9.3041, 10.4888,  7.1905, 10.1901,  9.0059],\n",
       "        [ 6.9247,  6.1737,  6.4403,  8.2655,  6.9839,  6.8822,  6.4180,  5.6645,\n",
       "          5.3719,  8.3474,  7.0873,  8.0365,  7.8363,  7.1742,  5.8870,  9.0458,\n",
       "          6.4265,  7.0792,  7.2046,  7.5532,  8.1640,  6.8719,  8.0280,  8.3601,\n",
       "          6.4268,  7.4730,  6.8686,  6.8999,  6.8934,  7.1186,  7.6609,  7.9261,\n",
       "          5.7581,  5.7721,  7.5281,  6.8642,  7.7644,  4.9589,  7.3293,  7.0197],\n",
       "        [ 8.6656,  7.3746,  8.0126,  9.3667,  7.4134,  7.6672,  8.0859,  7.0374,\n",
       "          6.4446,  9.3502,  9.8229,  8.9388,  8.0177,  8.7325,  8.2413,  8.5308,\n",
       "          7.0007,  8.3674,  7.4808,  8.9930,  8.7478,  7.8300,  9.0407,  8.4333,\n",
       "          6.6682,  9.0142,  8.0351,  8.5773,  8.0006,  8.2948,  9.3663,  8.3256,\n",
       "          8.1557,  6.0888,  8.6257,  7.7105,  9.0474,  6.3167,  8.4877,  7.0631],\n",
       "        [ 6.4734,  5.0284,  5.9130,  7.3672,  6.7254,  6.3679,  5.7966,  6.0959,\n",
       "          4.9849,  8.0865,  6.6345,  7.2593,  7.4006,  7.0689,  6.0541,  7.5443,\n",
       "          5.9727,  5.9668,  6.4069,  7.1511,  7.2495,  7.2539,  7.4999,  7.9492,\n",
       "          5.6448,  6.8376,  6.8958,  7.2745,  6.4727,  6.3752,  7.5977,  7.6549,\n",
       "          6.3786,  4.6239,  6.6600,  5.6001,  7.1803,  5.5642,  7.0953,  6.2217],\n",
       "        [ 6.6645,  6.1197,  6.5135,  8.3148,  7.4332,  6.6975,  7.6497,  7.5448,\n",
       "          5.0958,  8.0665,  7.6680,  8.2315,  8.1907,  7.6670,  7.1175,  7.8796,\n",
       "          6.0625,  7.0702,  7.6073,  7.5547,  7.3525,  7.7773,  8.2430,  7.9230,\n",
       "          5.5420,  7.3975,  6.7723,  7.0145,  7.2904,  7.3427,  8.8009,  7.1869,\n",
       "          6.8618,  4.7860,  6.9761,  6.7234,  7.5448,  5.9195,  7.8178,  6.5703],\n",
       "        [ 8.1392,  6.4524,  6.3473,  8.6934,  6.3650,  7.4589,  8.1994,  6.3864,\n",
       "          5.8052,  9.2832,  9.2251,  7.8362,  7.3391,  7.1002,  6.8628,  8.2663,\n",
       "          7.3789,  7.9508,  7.9966,  8.8020,  8.4288,  7.3698,  9.3626,  8.8673,\n",
       "          6.4659,  9.1068,  8.4285,  7.8298,  6.7929,  7.4079,  8.6388,  8.3508,\n",
       "          7.0016,  6.4055,  8.3256,  6.4544,  7.5097,  6.7376,  8.0216,  6.1042],\n",
       "        [ 7.1689,  6.4201,  6.4391,  9.9027,  8.1385,  8.0096,  7.5778,  7.2504,\n",
       "          6.7195,  9.8773,  8.7869,  8.7141,  8.7279,  8.2083,  7.8633,  9.5275,\n",
       "          7.8442,  7.9546,  7.7459,  8.5117,  9.1498,  7.5772,  9.4202,  9.0539,\n",
       "          7.5235,  9.1185,  8.1691,  9.0985,  7.3850,  7.8036,  9.8370,  8.7768,\n",
       "          7.8718,  5.7367,  8.8158,  7.0833,  8.6762,  6.7764,  7.7619,  8.3292],\n",
       "        [ 7.2827,  6.1284,  7.5318,  9.3764,  8.2285,  7.6255,  8.2074,  7.4922,\n",
       "          6.9870,  9.3246,  8.6271,  9.1111,  9.3637,  8.0855,  7.2523,  8.7768,\n",
       "          7.1381,  7.4461,  8.1331,  8.4442,  9.0392,  7.5857,  9.0240,  8.9231,\n",
       "          7.0355,  8.8042,  7.8989,  8.4351,  8.4806,  8.2658,  8.3236,  9.1874,\n",
       "          7.4427,  5.8925,  7.8002,  7.7593,  8.6422,  6.3051,  8.6683,  7.7212],\n",
       "        [ 7.7153,  5.3669,  6.8548,  7.0629,  6.7862,  6.7730,  7.1661,  5.7148,\n",
       "          4.8513,  7.4884,  8.9687,  7.2239,  7.9398,  6.4243,  6.3862,  7.7118,\n",
       "          6.2567,  6.8957,  7.6024,  9.1649,  8.0230,  7.0949,  9.1461,  8.3188,\n",
       "          6.7324,  7.6767,  7.7650,  7.7257,  5.9493,  6.8007,  8.7780,  7.5700,\n",
       "          6.1126,  5.6265,  6.7440,  6.2926,  8.3133,  5.6673,  7.1862,  6.5664]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "A = torch.rand(20, 30)\n",
    "B = torch.rand(40, 30)\n",
    "C = B # without clone (1692, 12040) is better in memory efficiency,\n",
    "      # with clone(2364, 12708)\n",
    "print(tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "A@C.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 100, 200])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex 12\n",
    "A = torch.rand(100, 200)\n",
    "B = torch.rand(100, 200)\n",
    "C = torch.rand(100, 200)\n",
    "D = torch.stack([A, B, C])\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]),\n",
       " tensor([[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0] == A, D[1] == B, D[2] == C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
